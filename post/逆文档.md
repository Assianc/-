在 `SimpleTfidfVectorizer` 中，`use_idf` 参数用于控制是否使用 IDF（逆文档频率）加权。当 `use_idf=True` 时，TF-IDF 变换会使用 IDF 权重，即考虑某个词在所有文档中的出现频率；当 `use_idf=False` 时，TF-IDF 变换只计算词频（TF），而不使用 IDF 权重。

### True 和 False 的含义
- `use_idf=True`：TF-IDF 变换将计算每个词的逆文档频率，并使用它来加权词频。这可以帮助降低在所有文档中频繁出现的词的权重，使得这些词对模型的影响减少。IDF 的公式为：

  \[
  \text{IDF}(t) = \log \left( \frac{N}{1 + df(t)} \right)
  \]

  其中 \( N \) 是文档总数，\( df(t) \) 是包含词 \( t \) 的文档数。
  
- `use_idf=False`：TF-IDF 变换只计算词频（TF），不使用逆文档频率（IDF）来加权。这样每个词的权重只由它在当前文档中出现的频率决定，不考虑它在其他文档中的出现频率。

### 更新后的代码

我们可以在 `SimpleGridSearchCV` 中添加 `use_idf` 参数进行搜索，并在 `SimpleTfidfVectorizer` 中使用它：

```python
class SimpleTfidfVectorizer:
    def __init__(self, use_idf=True):
        self.vocabulary_ = {}
        self.idf_ = {}
        self.use_idf = use_idf

    def fit(self, raw_documents):
        vocab = {}
        doc_count = {}
        total_docs = len(raw_documents)

        for doc in raw_documents:
            words = doc  # 使用 jieba 分词
            for word in words:
                if word not in vocab:
                    vocab[word] = len(vocab)
                    doc_count[word] = 1
                else:
                    doc_count[word] += 1

        self.vocabulary_ = vocab

        if self.use_idf:
            for word, count in doc_count.items():
                self.idf_[word] = np.log(total_docs / (1 + count))

        return self

    def transform(self, raw_documents):
        rows = []
        for doc in raw_documents:
            words = doc  # 使用 jieba 分词
            row = [0] * len(self.vocabulary_)
            word_count = {}
            for word in words:
                if word in self.vocabulary_:
                    word_count[word] = word_count.get(word, 0) + 1

            for word, count in word_count.items():
                if self.use_idf and word in self.idf_:
                    row[self.vocabulary_[word]] = count * self.idf_[word]
                else:
                    row[self.vocabulary_[word]] = count

            rows.append(row)
        return np.array(rows)

    def fit_transform(self, raw_documents):
        self.fit(raw_documents)
        return self.transform(raw_documents)
    
    def get_params(self, deep=True):
        return {"use_idf": self.use_idf}
    
    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self
```

### 超参数搜索中的进度条

为了在超参数搜索过程中添加进度条，可以在 `SimpleGridSearchCV` 的 `fit` 方法中使用 `tqdm`：

```python
class SimpleGridSearchCV:
    def __init__(self, estimator, param_grid, cv=3):
        self.estimator = estimator
        self.param_grid = param_grid
        self.cv = cv
        self.best_estimator_ = None
        self.best_params_ = None
        self.best_score_ = None

    def fit(self, X, y):
        from itertools import product
        param_combinations = list(product(*self.param_grid.values()))
        best_score = -np.inf
        best_params = None

        for param_set in tqdm(param_combinations, desc="超参数搜索"):
            params = dict(zip(self.param_grid.keys(), param_set))
            scores = []
            for train_idx, val_idx in self._k_fold_split(X, y, self.cv):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                model = SimpleNaiveBayes(**params)
                model.fit(X_train, y_train)
                scores.append(model.score(X_val, y_val))
            
            avg_score = np.mean(scores)
            if avg_score > best_score:
                best_score = avg_score
                best_params = params

        self.best_score_ = best_score
        self.best_params_ = best_params
        self.best_estimator_ = SimpleNaiveBayes(**best_params)
        self.best_estimator_.fit(X, y)

    def _k_fold_split(self, X, y, k):
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        fold_sizes = np.full(k, len(X) // k, dtype=int)
        fold_sizes[:len(X) % k] += 1
        current = 0
        for fold_size in fold_sizes:
            start, stop = current, current + fold_size
            val_idx = indices[start:stop]
            train_idx = np.concatenate([indices[:start], indices[stop:]])
            yield train_idx, val_idx
            current = stop
```

### 更新后的 `main` 函数

```python
import numpy as np
import pandas as pd
from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import multiprocessing as mp
from itertools import repeat

from naiveBayesCN import load_stop_words, loadCNDataSet, preprocess_doc
from simple_vectorizers import SimpleCountVectorizer, SimpleTfidfVectorizer, SimpleGridSearchCV

class SimpleNaiveBayes:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.classes_ = np.unique(y)
        n_classes = len(self.classes_)
        
        self.class_log_prior_ = np.zeros(n_classes)
        self.feature_log_prob_ = np.zeros((n_classes, n_features))
        
        for idx, c in enumerate(self.classes_):
            X_c = X[y == c]
            self.class_log_prior_[idx] = np.log(X_c.shape[0] / n_samples)
            self.feature_log_prob_[idx] = np.log((X_c.sum(axis=0) + self.alpha) / (X_c.sum() + self.alpha * n_features))
    
    def predict(self, X):
        jll = X @ self.feature_log_prob_.T + self.class_log_prior_
        return self.classes_[np.argmax(jll, axis=1)]
    
    def score(self, X, y):
        return np.mean(self.predict(X) == y)
    
    def get_params(self, deep=True):
        return {"alpha": self.alpha}
    
    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self

def main():
    stop_words = load_stop_words()

    # 数据下采样
    lines = 30000  # 数据量

    # 加载中文数据集并进行分词和停用词过滤
    listOposts, listClasses = loadCNDataSet(lines, stop_words)

    # 并行预处理文档
    with mp.Pool(mp.cpu_count()) as pool:
        preprocessed_docs = list(tqdm(pool.imap(preprocess_doc, listOposts),
                                      total=len(listOposts), desc='预处理文档'))

    # 划分数据集
    X_train, X_test, y_train, y_test = train_test_split(preprocessed_docs, listClasses, test_size=0.2, random_state=1)

    # 使用 SimpleCountVectorizer 或 SimpleTfidfVectorizer
    vectorizer = SimpleTfidfVectorizer()  # 可以切换为 SimpleCountVectorizer
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    # 使用 SimpleGridSearchCV 进行超参数搜索
    param_grid = {
        'alpha': [0.5, 1.0],
        'vectorizer__max_df': [0.75, 1.0],  # 针对 SimpleCountVectorizer
        'vectorizer__use_idf': [True, False]  # 针对 SimpleTfidfVectorizer
    }
    grid_search = SimpleGridSearchCV(SimpleNaiveBayes(), param_grid, cv=3)
    grid_search.fit(X_train_vec, y_train)

    # 获取最佳参数
    best_params = grid_search.best_params_
    print(f"最佳参数: {best_params}")

    # 使用最佳参数训练模型
    best_model = SimpleNaiveBayes(**best_params)
    best_model.fit(X_train_vec, y_train)

    # 预测
    y_pred = best_model.predict(X_test_vec)

    # 评估模型
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average

='weighted', zero_division=1)
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"准确率: {accuracy}")
    print(f"精确率: {precision}")
    print(f"召回率: {recall}")
    print(f"F1值: {f1}")

    # 输出结果
    with open('result/best_score.txt', 'w', encoding='utf-8') as file:
        file.write(f"准确率: {accuracy}\n")
        file.write(f"精确率: {precision}\n")
        file.write(f"召回率: {recall}\n")
        file.write(f"F1值: {f1}\n")

if __name__ == '__main__':
    main()
```

在这个代码中，`SimpleGridSearchCV` 在超参数搜索过程中使用 `tqdm` 显示进度条，并且可以搜索 `use_idf` 参数，控制是否在 TF-IDF 变换中使用 IDF 权重。
