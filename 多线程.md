```python
from multiprocessing import Pool
import nativeBayesCN as nbcn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
from itertools import repeat
import pandas as pd
import os


def main():
    # 加载停用词
    stop_words = nbcn.load_stop_words()

    # 加载数据集，并使用多进程预处理以加速
    lines = 100000  # 数据量可以根据实际情况调整
    listOposts, listClasses = nbcn.loadDataSet(stop_words, lines)

    # 多进程预处理文档
    with Pool(os.cpu_count()) as pool:
        preprocessed_docs = list(
            tqdm(pool.imap(nbcn.preprocess_doc, zip(listOposts, repeat(stop_words))), total=len(listOposts),
                 desc='预处理文档'))

    # 使用TF-IDF进行特征转换
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(preprocessed_docs)
    y = listClasses

    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

    # 训练朴素贝叶斯分类器
    clf = MultinomialNB()
    clf.fit(X_train, y_train)

    # 预测和评估
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"准确率: {accuracy}")
    print(f"精确率: {precision}")
    print(f"召回率: {recall}")
    print(f"F1值: {f1}")

    # 保存结果到txt文件
    with open('result/score.txt', 'w', encoding='utf-8') as file:
        file.write(f"准确率: {accuracy}\n")
        file.write(f"精确率: {precision}\n")
        file.write(f"召回率: {recall}\n")
        file.write(f"F1值: {f1}\n")


if __name__ == '__main__':
    main()
```

```python
import jieba
import multiprocessing as mp
from itertools import islice
from tqdm import tqdm

def load_stop_words():
    """
    加载停用词列表
    """
    stop_words = set()
    with open('./data/cnsmss/stopWord.txt', 'r', encoding='utf-8') as file:
        for line in file:
            stop_words.add(line.strip())
    return stop_words

def loadDataSet(stop_words, lines=5000):
    """
    读取中文数据集并进行预处理
    """
    postingList = []  # 存储文本
    classVec = []  # 存储标签
    with open('./data/cnsmss/80w.txt', 'r', encoding='utf-8') as file:
        dataSet = [line.strip().split('\t') for line in islice(file, lines)]
        for item in tqdm(dataSet, desc='加载数据集：'):
            # 检查数据格式是否正确，至少包含3个元素
            if len(item) >= 3:
                classVec.append(int(item[1]))  # 假设第2个元素是类别
                # 去除停用词
                words = jieba.lcut(item[2], cut_all=False)
                postingList.append([word for word in words if word not in stop_words])
            else:
                print(f"警告：数据行格式不正确，已跳过。原始行: '{item}'")
    return postingList, classVec

def preprocess_doc(args):
    """
    单个文档预处理函数，用于多进程调用
    """
    doc, stop_words = args
    return ' '.join(jieba.lcut(doc, cut_all=False) if isinstance(doc, str) else doc)  # 预处理文档并返回处理后的文本字符串

if __name__ == '__main__':
    
    stop_words = load_stop_words()
    listOposts, listClasses = loadDataSet(stop_words)
    print("Data loaded.")
```